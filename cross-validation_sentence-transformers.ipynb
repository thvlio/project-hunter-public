{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>title+desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>energy effiency</td>\n",
       "      <td>intensity control of led light points for high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waste management</td>\n",
       "      <td>instalación de planta trituradora de llantas. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transport</td>\n",
       "      <td>seguimiento y control ambiental a tecnologías ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>waste management</td>\n",
       "      <td>fortalecimiento programa de reciclaje inclusiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waste management</td>\n",
       "      <td>waste to energy project for blantyre city. reh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>energy effiency</td>\n",
       "      <td>decarbonisation of local authority maintained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>buildings</td>\n",
       "      <td>charlotte &amp; william bloomberg public library. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>energy effiency</td>\n",
       "      <td>bloomington green home improvement program. th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>water management</td>\n",
       "      <td>greater amman municipality (gam) - saqef al se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>water management</td>\n",
       "      <td>estrategia hidrica local. herramienta local de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1158 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  area                                         title+desc\n",
       "0      energy effiency  intensity control of led light points for high...\n",
       "1     waste management  instalación de planta trituradora de llantas. ...\n",
       "2            transport  seguimiento y control ambiental a tecnologías ...\n",
       "3     waste management  fortalecimiento programa de reciclaje inclusiv...\n",
       "4     waste management  waste to energy project for blantyre city. reh...\n",
       "...                ...                                                ...\n",
       "1300   energy effiency  decarbonisation of local authority maintained ...\n",
       "1301         buildings  charlotte & william bloomberg public library. ...\n",
       "1302   energy effiency  bloomington green home improvement program. th...\n",
       "1303  water management  greater amman municipality (gam) - saqef al se...\n",
       "1304  water management  estrategia hidrica local. herramienta local de...\n",
       "\n",
       "[1158 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the same routine for the untranslated dataset\n",
    "df_cdp_multi = pd.read_csv('csvs/cdp_clean.csv', sep=',').drop(columns=['Unnamed: 0'])[['area', 'title+desc']]\n",
    "\n",
    "area_energy = (df_cdp_multi['area'] == 'energy efficiency / retrofit') | (df_cdp_multi['area'] == 'energy efficiency (including public lighting)')\n",
    "df_cdp_multi.loc[area_energy, 'area'] = 'energy efficiency'\n",
    "\n",
    "area_waste = df_cdp_multi['area'] == 'waste management (including waste recycling)'\n",
    "df_cdp_multi.loc[area_waste, 'area'] = 'waste management'\n",
    "\n",
    "min_sample_count = 100\n",
    "area_count = pd.DataFrame(df_cdp_multi.value_counts(subset='area')).rename(columns={0: 'sample_count'})\n",
    "area_count = area_count.rename({0: 'count'}, axis='columns')\n",
    "areas_to_keep = area_count[area_count > min_sample_count].dropna().index.to_list()\n",
    "area_filter = df_cdp_multi['area'].isin(areas_to_keep)\n",
    "df_clean_multi = df_cdp_multi[area_filter]\n",
    "df_clean_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y.astype(float))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(torch.nn.Module):\n",
    "    def __init__(self, encoder, hidden_neurons, num_classes, encoder_trainable=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = encoder_trainable\n",
    "        self.encoder\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.fc1 = torch.nn.Linear(self.encoder.get_sentence_embedding_dimension(), hidden_neurons)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_neurons, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(self.encoder.encode(x))\n",
    "        x = F.relu(self.fc1(self.dropout1(x)))\n",
    "        x = F.softmax(self.fc2(self.dropout2(x)), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implements the model creation, training and deletion\n",
    "def fit_torch(x_train,\n",
    "              y_train,\n",
    "              x_val,\n",
    "              y_val,\n",
    "              classes,\n",
    "              model_type,\n",
    "              hidden_neurons,\n",
    "              epochs,\n",
    "              queue):\n",
    "    \n",
    "    # bring pre-trained model from tf hub as a layer\n",
    "    if model_type == 'labse':\n",
    "        encoder = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "    elif model_type == 'duse':\n",
    "        encoder = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "    elif model_type == 'minilm':\n",
    "        encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    elif model_type == 'mpnet':\n",
    "        encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "    # create the model with the parameters given\n",
    "    # model.max_seq_length = 512\n",
    "    model = CustomNet(encoder, hidden_neurons, len(classes))\n",
    "\n",
    "    # setup loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # create dataloader\n",
    "    train_set = CustomDataset(x_train, y_train)\n",
    "    val_set = CustomDataset(x_val, y_val)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # train the model\n",
    "    best_vloss = 1_000_000.0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        model.train(True)\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, pred_idx = outputs.max(1)\n",
    "            _, true_idx = labels.max(1)\n",
    "            correct += (pred_idx == true_idx).sum().item()\n",
    "        avg_loss = running_loss / (i + 1)\n",
    "        accuracy = correct / len(train_set)\n",
    "\n",
    "        model.train(False)\n",
    "        vcorrect = 0\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "            _, vpred_idx = voutputs.max(1)\n",
    "            _, vtrue_idx = vlabels.max(1)\n",
    "            vcorrect += (vpred_idx == vtrue_idx).sum().item()\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        vaccuracy = vcorrect / len(val_set)\n",
    "\n",
    "        # print(f'loss train {avg_loss:.4f} - acc train {accuracy:.4f} - loss validation {avg_vloss:.4f} - acc validation {vaccuracy:.4f}')\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        # if avg_vloss < best_vloss:\n",
    "        #     best_vloss = avg_vloss\n",
    "        #     model_path = f'models/model_{epoch}'\n",
    "        #     torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # return the last validation accuracy\n",
    "    val_acc = vaccuracy\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    queue.put((val_acc, train_time, num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function implements cross validation for keras classifiers (which are not supported by scikit-learn)\n",
    "def cross_validate_torch(df_dataset, \n",
    "                         num_folds,\n",
    "                         input_column,\n",
    "                         output_column,\n",
    "                         model_type,\n",
    "                         hidden_neurons,\n",
    "                         epochs):\n",
    "    \n",
    "    # multiprocessing queue for retrieving the fit_keras result\n",
    "    queue = mp.Queue()\n",
    "    \n",
    "    # x is input, y is output\n",
    "    x = df_dataset[input_column].to_numpy()\n",
    "    y = df_dataset[output_column].to_numpy()\n",
    "\n",
    "    # one hot encode the output\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(y)\n",
    "\n",
    "    # manually create the folds and iterate through them\n",
    "    cv_metrics = {\n",
    "        'score': [],\n",
    "        'time': [],\n",
    "        'params': []\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for k, (train, val) in enumerate(skf.split(x, y)):\n",
    "\n",
    "        p = mp.Process(target=fit_torch, args=(x[train],\n",
    "                                               label_binarizer.transform(y[train]),\n",
    "                                               x[val],\n",
    "                                               label_binarizer.transform(y[val]),\n",
    "                                               np.unique(y),\n",
    "                                               model_type,\n",
    "                                               hidden_neurons,\n",
    "                                               epochs,\n",
    "                                               queue))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        \n",
    "        # fit_torch(x[train],\n",
    "        #           label_binarizer.transform(y[train]),\n",
    "        #           x[val],\n",
    "        #           label_binarizer.transform(y[val]),\n",
    "        #           np.unique(y),\n",
    "        #           model_type,\n",
    "        #           hidden_neurons,\n",
    "        #           epochs,\n",
    "        #           queue)\n",
    "\n",
    "        # retrieve the validation accuracy from the queue\n",
    "        val_acc, train_time, model_params = queue.get()\n",
    "        cv_metrics['score'].append(val_acc)\n",
    "        cv_metrics['time'].append(train_time)\n",
    "        cv_metrics['params'].append(model_params)\n",
    "        print(f'col: {input_column}, model: {model_type}, neurons: {hidden_neurons}, epochs: {epochs}, fold: {k}, acc: {val_acc:.4f}, time: {int(train_time)}')\n",
    "\n",
    "    return cv_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete results folder if existent\n",
    "# create new results folder\n",
    "if os.path.exists('results_hf'):\n",
    "    shutil.rmtree('results_hf')\n",
    "os.makedirs('results_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter sets\n",
    "param_sets = [\n",
    "    # model_type, hidden_neurons, epochs\n",
    "    (True, 'labse', 28, 50),\n",
    "    (True, 'duse', 24, 50),\n",
    "    (True, 'minilm', 20, 50),\n",
    "    (True, 'mpnet', 28, 50)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 0, acc: 0.8017, time: 430\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 1, acc: 0.7629, time: 421\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 2, acc: 0.7198, time: 401\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 3, acc: 0.7749, time: 394\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 4, acc: 0.7619, time: 388\n",
      "total time: 2059\n",
      "\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 0, acc: 0.7931, time: 98\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 1, acc: 0.7672, time: 98\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 2, acc: 0.7284, time: 98\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 3, acc: 0.7922, time: 98\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 4, acc: 0.7532, time: 98\n",
      "total time: 497\n",
      "\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 0, acc: 0.7586, time: 77\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 1, acc: 0.7543, time: 77\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 2, acc: 0.7069, time: 77\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 3, acc: 0.7403, time: 77\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 4, acc: 0.7489, time: 77\n",
      "total time: 396\n",
      "\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 0, acc: 0.7629, time: 182\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 1, acc: 0.7974, time: 182\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 2, acc: 0.7543, time: 182\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 3, acc: 0.8095, time: 182\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 4, acc: 0.7706, time: 202\n",
      "total time: 948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the loop implement the grid search to find the best parameters for the model\n",
    "queue = mp.Queue()\n",
    "\n",
    "for _, model_type, hidden_neurons, epochs in param_sets:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    df_dataset = df_clean_multi\n",
    "\n",
    "    cv_metrics = cross_validate_torch(df_dataset=df_dataset,\n",
    "                                      num_folds=5,\n",
    "                                      input_column='title+desc',\n",
    "                                      output_column='area',\n",
    "                                      model_type=model_type,\n",
    "                                      hidden_neurons=hidden_neurons,\n",
    "                                      epochs=epochs)\n",
    "    \n",
    "    cv_total_time = time.time() - t0\n",
    "    \n",
    "    print(f'total time: {int(cv_total_time)}\\n')\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_metrics['score'])\n",
    "    mean_cv_time = np.mean(cv_metrics['time'])\n",
    "    cv_params = np.mean(cv_metrics['params'])\n",
    "    \n",
    "    file_path = os.path.join('results_hf', f'{model_type}_{hidden_neurons}_{epochs}')\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f'{mean_cv_score:.4f} {int(mean_cv_time)} {cv_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files generated by the grid search and put results on a table\n",
    "torch_columns = ['model_type', 'hidden_neurons', 'epochs', 'cv_score', 'cv_time', 'trainable_params']\n",
    "df_results_torch = pd.DataFrame(columns=torch_columns)\n",
    "\n",
    "for file_path in sorted(os.listdir('results_hf')):\n",
    "    with open(os.path.join('results_hf', file_path), 'r') as f:\n",
    "        parameters = file_path.split('_')\n",
    "        results = f.read()\n",
    "        cv_score, cv_time, trainable_params = results.split(' ')\n",
    "        data = parameters + [float(cv_score), float(cv_time), float(trainable_params)]\n",
    "        df_row = pd.DataFrame(data=[data], columns=torch_columns)\n",
    "        df_results_torch = pd.concat([df_results_torch, df_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_params(row):\n",
    "    row['trainable_params'] = f'{int(row[\"trainable_params\"]/1e6)} M'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model_type   |   hidden_neurons |   cv_score | trainable_params   |\n",
      "|:-------------|-----------------:|-----------:|:-------------------|\n",
      "| duse         |               24 |     0.7668 | 135 M              |\n",
      "| labse        |               28 |     0.7643 | 471 M              |\n",
      "| minilm       |               20 |     0.7418 | 117 M              |\n",
      "| mpnet        |               28 |     0.7789 | 278 M              |\n"
     ]
    }
   ],
   "source": [
    "# group by each parameter\n",
    "print(df_results_torch.groupby(['model_type', 'hidden_neurons'])[['cv_score', 'trainable_params']].mean().apply(format_params, axis=1).reset_index().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model_type   |   cv_score | trainable_params   |\n",
      "|:-------------|-----------:|:-------------------|\n",
      "| duse         |     0.7668 | 135 M              |\n",
      "| labse        |     0.7643 | 471 M              |\n",
      "| minilm       |     0.7418 | 117 M              |\n",
      "| mpnet        |     0.7789 | 278 M              |\n"
     ]
    }
   ],
   "source": [
    "print(df_results_torch.groupby(['model_type'])[['cv_score', 'trainable_params']].mean().apply(format_params, axis=1).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vignoli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1257ee0ebd65c7b37e80f73fc04166e1339345524d044862985f662e63191f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
