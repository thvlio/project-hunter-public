{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>title+desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>energy effiency</td>\n",
       "      <td>intensity control of led light points for high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waste management</td>\n",
       "      <td>instalación de planta trituradora de llantas. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transport</td>\n",
       "      <td>seguimiento y control ambiental a tecnologías ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>waste management</td>\n",
       "      <td>fortalecimiento programa de reciclaje inclusiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waste management</td>\n",
       "      <td>waste to energy project for blantyre city. reh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>water management</td>\n",
       "      <td>restauracion de las cuencas de agua. se preten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>energy effiency</td>\n",
       "      <td>sustainable tourism development program in iga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>water management</td>\n",
       "      <td>caminho das águas. o projeto prevê a a realiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>waste management</td>\n",
       "      <td>implantação do aterro sanitário em forma de co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>water management</td>\n",
       "      <td>proteção e recuperação de nascentes do ribeirã...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 area                                         title+desc\n",
       "0     energy effiency  intensity control of led light points for high...\n",
       "1    waste management  instalación de planta trituradora de llantas. ...\n",
       "2           transport  seguimiento y control ambiental a tecnologías ...\n",
       "3    waste management  fortalecimiento programa de reciclaje inclusiv...\n",
       "4    waste management  waste to energy project for blantyre city. reh...\n",
       "..                ...                                                ...\n",
       "482  water management  restauracion de las cuencas de agua. se preten...\n",
       "483   energy effiency  sustainable tourism development program in iga...\n",
       "484  water management  caminho das águas. o projeto prevê a a realiza...\n",
       "486  waste management  implantação do aterro sanitário em forma de co...\n",
       "488  water management  proteção e recuperação de nascentes do ribeirã...\n",
       "\n",
       "[1560 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the same routine for the untranslated dataset\n",
    "df_cdp_multi = pd.read_csv('csvs/cdp_clean.csv', sep=',').drop(columns=['Unnamed: 0'])\n",
    "df_es_multi = pd.read_csv('csvs/es_clean.csv', sep=',').drop(columns=['Unnamed: 0'])\n",
    "df_joined_multi = pd.concat([df_cdp_multi, df_es_multi], axis=0)[['area', 'title+desc']]\n",
    "\n",
    "area_energy = (df_joined_multi['area'] == 'energy efficiency / retrofit') | (df_joined_multi['area'] == 'energy efficiency (including public lighting)')\n",
    "df_joined_multi.loc[area_energy, 'area'] = 'energy effiency'\n",
    "\n",
    "area_waste = df_joined_multi['area'] == 'waste management (including waste recycling)'\n",
    "df_joined_multi.loc[area_waste, 'area'] = 'waste management'\n",
    "\n",
    "min_sample_count = 100\n",
    "area_count = pd.DataFrame(df_joined_multi.value_counts(subset='area')).rename(columns={0: 'sample_count'})\n",
    "area_count = area_count.rename({0: 'count'}, axis='columns')\n",
    "areas_to_keep = area_count[area_count > min_sample_count].dropna().index.to_list()\n",
    "area_filter = df_joined_multi['area'].isin(areas_to_keep)\n",
    "df_clean_multi = df_joined_multi[area_filter]\n",
    "df_clean_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y.astype(float))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(torch.nn.Module):\n",
    "    def __init__(self, encoder, hidden_neurons, num_classes, encoder_trainable=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = encoder_trainable\n",
    "        self.encoder\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.fc1 = torch.nn.Linear(self.encoder.get_sentence_embedding_dimension(), hidden_neurons)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_neurons, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(self.encoder.encode(x))\n",
    "        x = F.relu(self.fc1(self.dropout1(x)))\n",
    "        x = F.softmax(self.fc2(self.dropout2(x)), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implements the model creation, training and deletion\n",
    "def fit_torch(x_train,\n",
    "              y_train,\n",
    "              x_val,\n",
    "              y_val,\n",
    "              classes,\n",
    "              model_type,\n",
    "              hidden_neurons,\n",
    "              epochs,\n",
    "              queue):\n",
    "    \n",
    "    # bring pre-trained model from tf hub as a layer\n",
    "    if model_type == 'labse':\n",
    "        encoder = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "    elif model_type == 'duse':\n",
    "        encoder = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "    elif model_type == 'minilm':\n",
    "        encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    elif model_type == 'mpnet':\n",
    "        encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "    # create the model with the parameters given\n",
    "    # model.max_seq_length = 512\n",
    "    model = CustomNet(encoder, hidden_neurons, len(classes))\n",
    "\n",
    "    # setup loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # create dataloader\n",
    "    train_set = CustomDataset(x_train, y_train)\n",
    "    val_set = CustomDataset(x_val, y_val)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # train the model\n",
    "    best_vloss = 1_000_000.0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        model.train(True)\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, pred_idx = outputs.max(1)\n",
    "            _, true_idx = labels.max(1)\n",
    "            correct += (pred_idx == true_idx).sum().item()\n",
    "        avg_loss = running_loss / (i + 1)\n",
    "        accuracy = correct / len(train_set)\n",
    "\n",
    "        model.train(False)\n",
    "        vcorrect = 0\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "            _, vpred_idx = voutputs.max(1)\n",
    "            _, vtrue_idx = vlabels.max(1)\n",
    "            vcorrect += (vpred_idx == vtrue_idx).sum().item()\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        vaccuracy = vcorrect / len(val_set)\n",
    "\n",
    "        # print(f'loss train {avg_loss:.4f} - acc train {accuracy:.4f} - loss validation {avg_vloss:.4f} - acc validation {vaccuracy:.4f}')\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        # if avg_vloss < best_vloss:\n",
    "        #     best_vloss = avg_vloss\n",
    "        #     model_path = f'models/model_{epoch}'\n",
    "        #     torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # return the last validation accuracy\n",
    "    val_acc = vaccuracy\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    queue.put((val_acc, train_time, num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function implements cross validation for keras classifiers (which are not supported by scikit-learn)\n",
    "def cross_validate_torch(df_dataset, \n",
    "                         num_folds,\n",
    "                         input_column,\n",
    "                         output_column,\n",
    "                         model_type,\n",
    "                         hidden_neurons,\n",
    "                         epochs):\n",
    "    \n",
    "    # multiprocessing queue for retrieving the fit_keras result\n",
    "    queue = mp.Queue()\n",
    "    \n",
    "    # x is input, y is output\n",
    "    x = df_dataset[input_column].to_numpy()\n",
    "    y = df_dataset[output_column].to_numpy()\n",
    "\n",
    "    # one hot encode the output\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(y)\n",
    "\n",
    "    # manually create the folds and iterate through them\n",
    "    cv_metrics = {\n",
    "        'score': [],\n",
    "        'time': [],\n",
    "        'params': []\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for k, (train, val) in enumerate(skf.split(x, y)):\n",
    "\n",
    "        p = mp.Process(target=fit_torch, args=(x[train],\n",
    "                                               label_binarizer.transform(y[train]),\n",
    "                                               x[val],\n",
    "                                               label_binarizer.transform(y[val]),\n",
    "                                               np.unique(y),\n",
    "                                               model_type,\n",
    "                                               hidden_neurons,\n",
    "                                               epochs,\n",
    "                                               queue))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        \n",
    "        # fit_torch(x[train],\n",
    "        #           label_binarizer.transform(y[train]),\n",
    "        #           x[val],\n",
    "        #           label_binarizer.transform(y[val]),\n",
    "        #           np.unique(y),\n",
    "        #           model_type,\n",
    "        #           hidden_neurons,\n",
    "        #           epochs,\n",
    "        #           queue)\n",
    "\n",
    "        # retrieve the validation accuracy from the queue\n",
    "        val_acc, train_time, model_params = queue.get()\n",
    "        cv_metrics['score'].append(val_acc)\n",
    "        cv_metrics['time'].append(train_time)\n",
    "        cv_metrics['params'].append(model_params)\n",
    "        print(f'col: {input_column}, model: {model_type}, neurons: {hidden_neurons}, epochs: {epochs}, fold: {k}, acc: {val_acc:.4f}, time: {int(train_time)}')\n",
    "\n",
    "    return cv_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete results folder if existent\n",
    "# create new results folder\n",
    "if os.path.exists('results_hf'):\n",
    "    shutil.rmtree('results_hf')\n",
    "os.makedirs('results_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter sets\n",
    "param_sets = [\n",
    "    # model_type, hidden_neurons, epochs\n",
    "    (True, 'labse', 28, 50),\n",
    "    (True, 'duse', 24, 50),\n",
    "    (True, 'minilm', 20, 50),\n",
    "    (True, 'mpnet', 28, 50)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 0, acc: 0.7853, time: 566\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 1, acc: 0.8205, time: 564\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 2, acc: 0.8205, time: 562\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 3, acc: 0.7340, time: 538\n",
      "col: title+desc, model: labse, neurons: 28, epochs: 50, fold: 4, acc: 0.7564, time: 518\n",
      "total time: 2769\n",
      "\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 0, acc: 0.7885, time: 135\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 1, acc: 0.7917, time: 145\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 2, acc: 0.8205, time: 145\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 3, acc: 0.7404, time: 145\n",
      "col: title+desc, model: duse, neurons: 24, epochs: 50, fold: 4, acc: 0.7724, time: 145\n",
      "total time: 722\n",
      "\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 0, acc: 0.7821, time: 115\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 1, acc: 0.7917, time: 115\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 2, acc: 0.8173, time: 115\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 3, acc: 0.7949, time: 107\n",
      "col: title+desc, model: minilm, neurons: 20, epochs: 50, fold: 4, acc: 0.7853, time: 107\n",
      "total time: 571\n",
      "\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 0, acc: 0.8237, time: 252\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 1, acc: 0.8237, time: 252\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 2, acc: 0.8173, time: 252\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 3, acc: 0.7981, time: 252\n",
      "col: title+desc, model: mpnet, neurons: 28, epochs: 50, fold: 4, acc: 0.8109, time: 252\n",
      "total time: 1275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the loop implement the grid search to find the best parameters for the model\n",
    "queue = mp.Queue()\n",
    "\n",
    "for _, model_type, hidden_neurons, epochs in param_sets:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    df_dataset = df_clean_multi\n",
    "\n",
    "    cv_metrics = cross_validate_torch(df_dataset=df_dataset,\n",
    "                                      num_folds=5,\n",
    "                                      input_column='title+desc',\n",
    "                                      output_column='area',\n",
    "                                      model_type=model_type,\n",
    "                                      hidden_neurons=hidden_neurons,\n",
    "                                      epochs=epochs)\n",
    "    \n",
    "    cv_total_time = time.time() - t0\n",
    "    \n",
    "    print(f'total time: {int(cv_total_time)}\\n')\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_metrics['score'])\n",
    "    mean_cv_time = np.mean(cv_metrics['time'])\n",
    "    cv_params = np.mean(cv_metrics['params'])\n",
    "    \n",
    "    file_path = os.path.join('results_hf', f'{model_type}_{hidden_neurons}_{epochs}')\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f'{mean_cv_score:.4f} {int(mean_cv_time)} {cv_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# read the files generated by the grid search and put results on a table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m keras_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhidden_neurons\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcv_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcv_time\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrainable_params\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m df_results_keras \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mkeras_columns)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m file_path \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39mresults_hf\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m      6\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mresults_hf\u001b[39m\u001b[39m'\u001b[39m, file_path), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# read the files generated by the grid search and put results on a table\n",
    "keras_columns = ['model_type', 'hidden_neurons', 'epochs', 'cv_score', 'cv_time', 'trainable_params']\n",
    "df_results_keras = pd.DataFrame(columns=keras_columns)\n",
    "\n",
    "for file_path in sorted(os.listdir('results_hf')):\n",
    "    with open(os.path.join('results_hf', file_path), 'r') as f:\n",
    "        parameters = file_path.split('_')\n",
    "        results = f.read()\n",
    "        cv_score, cv_time, trainable_params = results.split(' ')\n",
    "        data = parameters + [float(cv_score), float(cv_time), float(trainable_params)]\n",
    "        df_row = pd.DataFrame(data=[data], columns=keras_columns)\n",
    "        df_results_keras = pd.concat([df_results_keras, df_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_params(row):\n",
    "    row['trainable_params'] = f'{int(row[\"trainable_params\"]/1e6)} M'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# group by each parameter\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(df_results_keras\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhidden_neurons\u001b[39m\u001b[39m'\u001b[39m])[[\u001b[39m'\u001b[39m\u001b[39mcv_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mapply(format_params, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mto_markdown(index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_results_keras' is not defined"
     ]
    }
   ],
   "source": [
    "# group by each parameter\n",
    "print(df_results_keras.groupby(['model_type', 'hidden_neurons'])[['cv_score', 'params']].mean().apply(format_params, axis=1).reset_index().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model_type   |   cv_score |   cv_time | trainable_params   |\n",
      "|:-------------|-----------:|----------:|:-------------------|\n",
      "| duse         |     0.7827 |       143 | 0.01 M             |\n",
      "| labse        |     0.7833 |       550 | 0.02 M             |\n",
      "| minilm       |     0.7942 |       112 | 0.01 M             |\n",
      "| mpnet        |     0.8147 |       252 | 0.02 M             |\n"
     ]
    }
   ],
   "source": [
    "print(df_results_keras.groupby(['model_type'])[['cv_score', 'params']].mean().apply(format_params, axis=1).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vignoli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1257ee0ebd65c7b37e80f73fc04166e1339345524d044862985f662e63191f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
